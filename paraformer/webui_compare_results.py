#!/usr/bin/env python3
# -*- encoding: utf-8 -*-

import streamlit as st
import pandas as pd
import os
import re
import matplotlib as mpl
import plotly.graph_objects as go
import hashlib

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="è¯­éŸ³è¯†åˆ«æ¨¡å‹å¯¹æ¯”åˆ†æ",
    page_icon="ğŸ¤",
    layout="wide",
    initial_sidebar_state="expanded",
)

# è®¾ç½®ä¸­æ–‡å­—ä½“
mpl.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial']
mpl.rcParams['axes.unicode_minus'] = False


# ä¿®æ”¹åçš„è§£æå‡½æ•°ï¼Œå¯ä»¥æ¥å—æ–‡ä»¶å¯¹è±¡æˆ–æ–‡ä»¶è·¯å¾„
def parse_results_file(file_input):
    """
    è§£ææ¨¡å‹å¯¹æ¯”ç»“æœæ–‡ä»¶ (å¯æ¥å—æ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡)
    è¿”å›ï¼š
    1. æ¨¡å‹åç§°åˆ—è¡¨ (åŸºäºæ‘˜è¦ç»Ÿè®¡)
    2. æ¯ä¸ªæ–‡ä»¶çš„è¯¦ç»†ç»“æœå­—å…¸
    3. æ€»ä½“ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    content = None
    file_source_info = "æœªçŸ¥æ¥æº"  # For error messages

    # Check if input is a Streamlit UploadedFile object
    if hasattr(file_input, 'getvalue') and hasattr(file_input, 'name'):
        file_source_info = f"ä¸Šä¼ çš„æ–‡ä»¶ '{file_input.name}'"
        try:
            content_bytes = file_input.getvalue()
            content = content_bytes.decode('utf-8')
        except Exception as e:
            st.error(f"è¯»å– {file_source_info} å†…å®¹å¤±è´¥: {e}")
            return None, None, None, None
    elif isinstance(file_input, str):
        file_source_info = f"æ–‡ä»¶è·¯å¾„ '{file_input}'"
        if not os.path.exists(file_input):
            st.error(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_input}")
            return None, None, None, None
        try:
            with open(file_input, 'r', encoding='utf-8') as f:
                content = f.read()
                with open(file_input, 'rb') as fb:
                    content_bytes = fb.read()
        except Exception as e:
            st.error(f"è¯»å– {file_source_info} å¤±è´¥: {e}")
            return None, None, None, None
    else:
        st.error("æ— æ•ˆçš„æ–‡ä»¶è¾“å…¥ç±»å‹")
        return None, None, None, None

    if content is None:
        st.error(f"æ— æ³•ä» {file_source_info} è·å–å†…å®¹ã€‚")
        return None, None, None, None

    content_hash = hashlib.md5(content_bytes).hexdigest()

    parts = content.split("=== æµ‹è¯•ç»“æœç»Ÿè®¡ ===")
    if len(parts) < 2:
        st.error(f"{file_source_info} æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ç»Ÿè®¡æ‘˜è¦éƒ¨åˆ†")
        return None, None, None, content_hash

    detail_part = parts[0]
    summary_part = parts[1]

    file_results = []
    model_names_from_detail = set()

    file_pattern = r'\n\[\d+/\d+\] æ–‡ä»¶: (.+)\nå‚è€ƒæ–‡æœ¬: (.+)\nå‚è€ƒæ–‡æœ¬\(æ— æ ‡ç‚¹\): (.+)\n(.*?)-{80}'
    file_blocks = re.findall(file_pattern, detail_part, re.DOTALL)

    for file_block in file_blocks:
        file_name, ref_text, ref_text_no_punc, model_results_text = file_block
        file_result = {
            'file_name': file_name,
            'reference_text': ref_text,
            'reference_text_no_punc': ref_text_no_punc,
            'models': {}
        }

        model_pattern = r'(\S+) è¯†åˆ«ç»“æœ: (.+)\n\1 CER: ([\d.]+)%'
        model_matches = re.findall(model_pattern, model_results_text)

        for model_match in model_matches:
            model_name, rec_text, cer = model_match
            model_names_from_detail.add(model_name)
            file_result['models'][model_name] = {
                'text': rec_text,
                'cer': float(cer)
            }

        file_results.append(file_result)

    stats = {}
    total_files_match = re.search(r'æ€»æµ‹è¯•æ•°æ®æ•°é‡: (\d+)', summary_part)
    total_time_match = re.search(r'æ•´ä½“ä»»åŠ¡æ€»è€—æ—¶: ([\d.]+) ç§’', summary_part)

    total_files = 0 
    if total_files_match:
        total_files = int(total_files_match.group(1))
        stats['total_files'] = total_files 
    else:
        st.warning("æœªèƒ½ä»æ‘˜è¦ä¸­è§£æå‡º 'æ€»æµ‹è¯•æ•°æ®æ•°é‡'") 

    if total_time_match:
        stats['total_time'] = float(total_time_match.group(1))
    else:
         st.warning("æœªèƒ½ä»æ‘˜è¦ä¸­è§£æå‡º 'æ•´ä½“ä»»åŠ¡æ€»è€—æ—¶'") 

    model_pattern = r'(\S+):\næˆåŠŸå¤„ç†æ•°é‡: (\d+)/\d+\nå¹³å‡å­—é”™ç‡\(CER\): ([\d.]+)%\nå¹³å‡è¯†åˆ«è€—æ—¶\(å•æ¡\): ([\d.]+)ç§’'
    model_matches = re.findall(model_pattern, summary_part)

    stats['models'] = {}
    model_names_from_summary = [] 

    for model_match in model_matches:
        model_name, success_count_str, cer_str, avg_time_str = model_match
        model_names_from_summary.append(model_name)
        success_count = int(success_count_str)
        cer = float(cer_str)
        avg_time = float(avg_time_str)

        recognition_rate = None 
        if total_files > 0:
            recognition_rate = (success_count / total_files) * 100.0
        stats['models'][model_name] = {
            'success_count': success_count,
            'cer': cer,
            'avg_time': avg_time,
            'recognition_rate': recognition_rate 
        }

    final_model_names = model_names_from_summary

    return final_model_names, file_results, stats, content_hash


# ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡è¡¨æ ¼
def generate_summary_table(stats):
    """ç”Ÿæˆæ¨¡å‹æ€§èƒ½æ±‡æ€»ç»Ÿè®¡è¡¨æ ¼"""
    if 'models' not in stats or not stats['models']:
        return None
    data = []
    for model_name, model_stats in stats['models'].items():
        cer = model_stats['cer']
        accuracy = 100.0 - cer
        data.append({
            'æ¨¡å‹åç§°': model_name,
            'æˆåŠŸè¯†åˆ«æ•°é‡': model_stats['success_count'],
            'å¹³å‡å­—é”™ç‡(CER)': f"{cer:.2f}%",
            'å¹³å‡æ­£ç¡®ç‡(Accuracy)': f"{accuracy:.2f}%", 
            'å¹³å‡è¯†åˆ«è€—æ—¶(ç§’/æ¡)': f"{model_stats['avg_time']:.3f}",
            # Format recognition rate, handle None case
            'è¯†åˆ«ç‡ (%)': f"{model_stats.get('recognition_rate', 0):.2f}%" if model_stats.get('recognition_rate') is not None else 'N/A'
        })

    columns_order = [
        'æ¨¡å‹åç§°',
        'æˆåŠŸè¯†åˆ«æ•°é‡',
        'å¹³å‡å­—é”™ç‡(CER)',
        'å¹³å‡æ­£ç¡®ç‡(Accuracy)',
        'å¹³å‡è¯†åˆ«è€—æ—¶(ç§’/æ¡)',
        'è¯†åˆ«ç‡ (%)'
    ]
    df = pd.DataFrame(data)
    return df[columns_order] 


# ç”Ÿæˆæ¨¡å‹å¹³å‡CERæˆ–æ­£ç¡®ç‡æˆ–è¯†åˆ«ç‡å¯¹æ¯”æŸ±çŠ¶å›¾
def plot_model_avg_cer(stats, model_names, metric_type='cer'):
    """ç”Ÿæˆæ¨¡å‹å¹³å‡CERæˆ–æ­£ç¡®ç‡æˆ–è¯†åˆ«ç‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
    if 'models' not in stats or not stats['models']:
        return None

    data = []
    for model_name in model_names:
        if model_name in stats['models']:
            value = None
            if metric_type == 'cer':
                value = stats['models'][model_name]['cer']
            elif metric_type == 'accuracy':
                value = 100 - stats['models'][model_name]['cer']  # æ­£ç¡®ç‡ = 100 - CER
            elif metric_type == 'recognition_rate':
                # Use .get() with default 0 in case rate is None
                value = stats['models'][model_name].get('recognition_rate', 0)

            if value is not None:
                data.append({
                    'model': model_name,
                    'value': value
                })

    if not data:
        return None

    df = pd.DataFrame(data)
    df = df.sort_values(by='value', ascending=(metric_type == 'cer')) # Ascending for CER, Descending for others

    models = df['model'].tolist()
    values = df['value'].tolist()

    # Determine color scheme based on metric type
    if metric_type == 'cer':
        # Red is bad (high CER)
        colors = ['rgba(220,20,60,0.8)', 'rgba(255,69,0,0.8)', 'rgba(255,140,0,0.8)',
                  'rgba(255,215,0,0.8)', 'rgba(154,205,50,0.8)', 'rgba(34,139,34,0.8)']
        color_map = {model: colors[min(i, len(colors)-1)] for i, model in enumerate(reversed(models))} # Worse models get redder
    elif metric_type == 'accuracy' or metric_type == 'recognition_rate':
        # Green is good (high accuracy/rate)
        colors = ['rgba(34,139,34,0.8)', 'rgba(154,205,50,0.8)', 'rgba(255,215,0,0.8)',
                 'rgba(255,140,0,0.8)', 'rgba(255,69,0,0.8)', 'rgba(220,20,60,0.8)']
        color_map = {model: colors[min(i, len(colors)-1)] for i, model in enumerate(models)} # Better models get greener
    else:
        color_map = {model: 'rgba(31, 119, 180, 0.8)' for model in models} # Default blue if metric type unknown

    fig = go.Figure(data=[go.Bar(
        x=models,
        y=values,
        text=[f'{v:.2f}%' for v in values],
        textposition='auto',
        marker_color=[color_map[model] for model in models],
        hovertemplate='<b>%{x}</b><br>%{y:.2f}%<extra></extra>'
    )])

    # Determine titles based on metric type
    if metric_type == 'cer':
        title = 'æ¨¡å‹å¹³å‡å­—é”™ç‡(CER)å¯¹æ¯”'
        y_axis_title = 'å¹³å‡å­—é”™ç‡ CER (%)'
    elif metric_type == 'accuracy':
        title = 'æ¨¡å‹å¹³å‡æ­£ç¡®ç‡(Accuracy)å¯¹æ¯”'
        y_axis_title = 'å¹³å‡æ­£ç¡®ç‡ (%)'
    elif metric_type == 'recognition_rate':
        title = 'æ¨¡å‹è¯†åˆ«ç‡å¯¹æ¯”'
        y_axis_title = 'è¯†åˆ«ç‡ (%)'
    else:
        title = 'æ¨¡å‹æ€§èƒ½å¯¹æ¯”'
        y_axis_title = 'æ•°å€¼ (%)'

    fig.update_layout(
        title={
            'text': title,
            'y':0.9,
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top'},
        xaxis_title='æ¨¡å‹åç§°',
        yaxis_title=y_axis_title,
        yaxis_ticksuffix='%',
        bargap=0.2, # gap between bars of adjacent location coordinates.
        # yaxis_range=[0, max(values) * 1.1] # Adjust y-axis range slightly
        yaxis_range=[min(values)*0.9 if min(values) > 0 else 0, max(values) * 1.1] # Dynamic range
    )

    return fig


# ç”Ÿæˆæ¨¡å‹å¹³å‡CERæˆ–æ­£ç¡®ç‡æˆ–è¯†åˆ«ç‡å¯¹æ¯”æŠ˜çº¿å›¾
def plot_model_avg_cer_line(stats, model_names, metric_type='cer'):
    """ç”Ÿæˆæ¨¡å‹å¹³å‡CERæˆ–æ­£ç¡®ç‡æˆ–è¯†åˆ«ç‡å¯¹æ¯”æŠ˜çº¿å›¾"""
    if 'models' not in stats or not stats['models']:
        return None

    data = []
    for model_name in model_names:
        if model_name in stats['models']:
            value = None
            if metric_type == 'cer':
                value = stats['models'][model_name]['cer']
            elif metric_type == 'accuracy':
                value = 100 - stats['models'][model_name]['cer']
            elif metric_type == 'recognition_rate':
                value = stats['models'][model_name].get('recognition_rate', 0)

            if value is not None:
                data.append({
                    'model': model_name,
                    'value': value
                })

    if not data:
        return None

    df = pd.DataFrame(data)
    # æŠ˜çº¿å›¾é€šå¸¸æŒ‰åŸå§‹æ¨¡å‹é¡ºåºå±•ç¤ºï¼Œæˆ–è€…æŒ‰åç§°æ’åºï¼Ÿè¿™é‡ŒæŒ‰ä¼ å…¥çš„model_namesé¡ºåº
    df['model'] = pd.Categorical(df['model'], categories=model_names, ordered=True)
    df = df.sort_values('model')

    models = df['model'].tolist()
    values = df['value'].tolist()

    fig = go.Figure()

    # Determine line name and color based on metric type
    if metric_type == 'cer':
        name_text = 'å¹³å‡CER'
        line_color = '#FF5733' # Red/Orange for error
        colorscale = 'RdYlGn_r' # Red-Yellow-Green reversed (Red is high)
    elif metric_type == 'accuracy':
        name_text = 'å¹³å‡æ­£ç¡®ç‡'
        line_color = '#33C1FF' # Blue for accuracy
        colorscale = 'Viridis' # Green is high
    elif metric_type == 'recognition_rate':
        name_text = 'è¯†åˆ«ç‡'
        line_color = '#4CAF50' # Green for rate
        colorscale = 'Viridis' # Green is high
    else:
        name_text = 'æŒ‡æ ‡å€¼'
        line_color = '#666666' # Grey default
        colorscale = 'Plasma'

    fig.add_trace(go.Scatter(
        x=models,
        y=values,
        mode='lines+markers+text',
        name=name_text,
        line=dict(color=line_color, width=3),
        marker=dict(
            size=10,
            color=values,
            colorscale=colorscale,
            showscale=False, # Hide color scale bar
            line=dict(width=1, color='DarkSlateGrey')
        ),
        text=[f'{v:.2f}%' for v in values],
        textposition="top center",
        hovertemplate='<b>%{x}</b><br>%{y:.2f}%<extra></extra>'
    ))

    # Determine titles based on metric type
    if metric_type == 'cer':
        title = 'æ¨¡å‹å¹³å‡å­—é”™ç‡(CER)å¯¹æ¯”æŠ˜çº¿å›¾'
        y_axis_title = 'å¹³å‡å­—é”™ç‡ CER (%)'
    elif metric_type == 'accuracy':
        title = 'æ¨¡å‹å¹³å‡æ­£ç¡®ç‡(Accuracy)å¯¹æ¯”æŠ˜çº¿å›¾'
        y_axis_title = 'å¹³å‡æ­£ç¡®ç‡ (%)'
    elif metric_type == 'recognition_rate':
        title = 'æ¨¡å‹è¯†åˆ«ç‡å¯¹æ¯”æŠ˜çº¿å›¾'
        y_axis_title = 'è¯†åˆ«ç‡ (%)'
    else:
        title = 'æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ˜çº¿å›¾'
        y_axis_title = 'æ•°å€¼ (%)'


    min_value = min(values) * 0.9 if min(values) > 0 else 0
    max_value = max(values) * 1.1

    fig.update_layout(
        title={
            'text': title,
            'y':0.9,
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top'},
        xaxis_title="æ¨¡å‹åç§°",
        yaxis_title=y_axis_title,
        yaxis_range=[min_value, max_value],
        yaxis_ticksuffix='%',
        hovermode="x unified"
    )

    return fig


# ä¸»å‡½æ•°
def main():
    st.title("ğŸ¤ è¯­éŸ³è¯†åˆ«æ¨¡å‹å¯¹æ¯”åˆ†æ")

    if 'loaded_content_hash' not in st.session_state:
        st.session_state['loaded_content_hash'] = None
    if 'parsed_data' not in st.session_state:
        st.session_state['parsed_data'] = None

    with st.sidebar:
        st.header("åˆ†æè¯´æ˜")
        st.markdown("""
        æ­¤å·¥å…·ç”¨äºå¯è§†åŒ–åˆ†ææ¨¡å‹å¯¹æ¯”æµ‹è¯•ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
        - æ¨¡å‹æ€§èƒ½æ€»è§ˆï¼ˆå­—é”™ç‡ã€å¤„ç†æ—¶é—´ï¼‰
        - å¹³å‡å­—é”™ç‡/æ­£ç¡®ç‡å¯¹æ¯”å›¾
        """)
        
        st.markdown("---")
        st.header("é€‰æ‹©ç»“æœæ–‡ä»¶")

        uploaded_file = st.file_uploader(
            "ä¸Šä¼ ç»“æœæ–‡ä»¶è¿›è¡Œåˆ†æ",
            type=["txt"],
            key='result_uploader',
            help="ä¸Šä¼ æ–‡ä»¶åå°†è‡ªåŠ¨å¼€å§‹åˆ†æã€‚"
        )

    if uploaded_file is not None:
        model_names, file_results, stats, current_content_hash = parse_results_file(uploaded_file)

        if current_content_hash is not None and \
           (st.session_state.loaded_content_hash != current_content_hash or st.session_state.parsed_data is None):

            st.session_state.loaded_content_hash = current_content_hash

            if model_names is not None and file_results is not None and stats is not None:
                st.session_state.parsed_data = (model_names, file_results, stats)
            else:
                st.session_state.parsed_data = None
                st.warning("æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹å’Œæ ¼å¼ã€‚ä¸Šæ–¹å¯èƒ½æ˜¾ç¤ºå…·ä½“é”™è¯¯ä¿¡æ¯ã€‚")

        if st.session_state.parsed_data is not None:
            if st.session_state.loaded_content_hash == current_content_hash:
                model_names, file_results, stats = st.session_state.parsed_data

                if not model_names:
                    st.warning("æœªä»ç»“æœæ–‡ä»¶çš„æ‘˜è¦ä¸­è§£æå‡ºä»»ä½•æ¨¡å‹åç§°ã€‚")
                if not stats or 'models' not in stats or not stats['models']:
                    st.warning("æœªä»ç»“æœæ–‡ä»¶ä¸­è§£æå‡ºæœ‰æ•ˆçš„ç»Ÿè®¡æ‘˜è¦ã€‚")

                total_files = stats.get('total_files', 0)
                total_time = stats.get('total_time', 0)

                st.markdown(f"### æ–‡ä»¶: `{uploaded_file.name}`")
                st.markdown(f"## åŸºæœ¬ä¿¡æ¯")
                col1, col2, col3 = st.columns(3)
                col1.metric("æ€»æµ‹è¯•æ–‡ä»¶æ•°", f"{total_files}")
                col2.metric("æ€»æµ‹è¯•æ—¶é•¿", f"{total_time:.2f}ç§’")
                col3.metric("æ¨¡å‹æ•°é‡", f"{len(model_names)}")

                st.markdown("## æ¨¡å‹æ€§èƒ½æ€»è§ˆ")
                summary_table = generate_summary_table(stats)
                if summary_table is not None and not summary_table.empty:
                    st.dataframe(summary_table, use_container_width=True)
                else:
                    st.info("æ²¡æœ‰è¶³å¤Ÿçš„ç»Ÿè®¡æ•°æ®æ¥æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½æ€»è§ˆã€‚")

                # æ·»åŠ å•é€‰æ¡†é€‰æ‹©æ˜¾ç¤ºå­—é”™ç‡æˆ–æ­£ç¡®ç‡
                st.markdown("## æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
                metric_options = ["å­—é”™ç‡(CER)", "æ­£ç¡®ç‡(Accuracy)", "è¯†åˆ«ç‡"]
                selected_metric = st.radio("é€‰æ‹©æ˜¾ç¤ºæŒ‡æ ‡ï¼š", metric_options, horizontal=True, key='metric_radio')

                # æ ¹æ®é€‰æ‹©çš„æŒ‡æ ‡ç±»å‹æ˜¾ç¤ºç›¸åº”çš„å›¾è¡¨
                if selected_metric == "å­—é”™ç‡(CER)":
                    metric_type = 'cer'
                elif selected_metric == "æ­£ç¡®ç‡(Accuracy)":
                    metric_type = 'accuracy'
                elif selected_metric == "è¯†åˆ«ç‡":
                    metric_type = 'recognition_rate'
                else:
                    metric_type = 'cer' # Default to CER

                # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŸ±çŠ¶å›¾
                avg_fig = plot_model_avg_cer(stats, model_names, metric_type)
                if avg_fig:
                    st.plotly_chart(avg_fig, use_container_width=True)
                else:
                    st.info(f"æ— æ³•ç”Ÿæˆ{selected_metric}æŸ±çŠ¶å›¾ã€‚")
                
                # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ˜çº¿å›¾
                avg_line_fig = plot_model_avg_cer_line(stats, model_names, metric_type)
                if avg_line_fig:
                    st.plotly_chart(avg_line_fig, use_container_width=True)
                else:
                    st.info(f"æ— æ³•ç”Ÿæˆ{selected_metric}æŠ˜çº¿å›¾ã€‚")

    else:
        st.info("ğŸ‘ˆ è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ç»“æœæ–‡ä»¶è¿›è¡Œåˆ†æ")
        st.markdown("## æ¨¡å‹æ€§èƒ½æ€»è§ˆ")
        st.markdown("ä¸Šä¼ æ–‡ä»¶åå°†åœ¨æ­¤å¤„æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½ç»Ÿè®¡è¡¨æ ¼å’Œå›¾è¡¨...")
        if st.session_state.parsed_data is not None:
            st.session_state.parsed_data = None
            st.session_state.loaded_content_hash = None


if __name__ == "__main__":
    main()